{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD He INITIALIZATION METHOD\n",
    "def initialize_params_random(net_dims):\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    params = {}\n",
    "    L = len(net_dims)            # number of layers in the network, including input\n",
    "    # Initialize params for each layer of the network, excluding the input layer\n",
    "    for l in range(1, L):\n",
    "        params['W' + str(l)] = np.random.randn(net_dims[l], net_dims[l-1]) * 0.01\n",
    "        params['b' + str(l)] = np.zeros((net_dims[l], 1))\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 0.01788628,  0.0043651 ,  0.00096497, -0.01863493, -0.00277388],\n",
      "       [-0.00354759, -0.00082741, -0.00627001, -0.00043818, -0.00477218],\n",
      "       [-0.01313865,  0.00884622,  0.00881318,  0.01709573,  0.00050034],\n",
      "       [-0.00404677, -0.0054536 , -0.01546477,  0.00982367, -0.01101068]]), 'b1': array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]]), 'W2': array([[-0.01185047, -0.0020565 ,  0.01486148,  0.00236716],\n",
      "       [-0.01023785, -0.00712993,  0.00625245, -0.00160513],\n",
      "       [-0.00768836, -0.00230031,  0.00745056,  0.01976111]]), 'b2': array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]])}\n"
     ]
    }
   ],
   "source": [
    "#Tests\n",
    "params = initialize_params_random([5,4,3])\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params_xavier(net_dims):    \n",
    "    np.random.seed(3)\n",
    "    params = {}\n",
    "    L = len(net_dims)            # number of layers in the network, including input\n",
    "    # Initialize params for each layer of the network, excluding the input layer\n",
    "    for l in range(1, L):\n",
    "        params['W' + str(l)] = np.random.randn(net_dims[l], net_dims[l-1]) / np.sqrt(net_dims[l-1])\n",
    "        params['b' + str(l)] = np.zeros((net_dims[l], 1))\n",
    "        \n",
    "        assert(params['W' + str(l)].shape == (net_dims[l], net_dims[l-1]))\n",
    "        assert(params['b' + str(l)].shape == (net_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_params = initialize_params_xavier([2, 5, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_params) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 1.26475132,  0.30865908],\n",
       "        [ 0.06823401, -1.31768833],\n",
       "        [-0.19614308, -0.25085248],\n",
       "        [-0.05850706, -0.44335643],\n",
       "        [-0.03098412, -0.33744411]]),\n",
       " 'W2': array([[-0.58757818,  0.39561516,  0.39413741,  0.76454432,  0.02237573],\n",
       "        [-0.18097724, -0.24389238, -0.69160568,  0.43932807, -0.49241241],\n",
       "        [-0.52996892, -0.09196943,  0.66462575,  0.10586273, -0.45785063],\n",
       "        [-0.31886025,  0.27961805, -0.07178376, -0.34383407, -0.10287287],\n",
       "        [ 0.33319929,  0.88374361, -0.55638887, -0.28014216, -0.35945513]]),\n",
       " 'W3': array([[-1.08184688, -0.41313235, -0.45789116,  0.50265822, -0.05899384]]),\n",
       " 'b1': array([[ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]]),\n",
       " 'b2': array([[ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]]),\n",
       " 'b3': array([[ 0.]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### He Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params_he(net_dims):   \n",
    "    np.random.seed(3)\n",
    "    params = {}\n",
    "    L = len(net_dims)            # number of layers in the network, including input\n",
    "    # Initialize params for each layer of the network, excluding the input layer\n",
    "    for l in range(1, L):\n",
    "        params['W' + str(l)] = np.random.randn(net_dims[l], net_dims[l-1]) * np.sqrt(2 / net_dims[l-1])\n",
    "        params['b' + str(l)] = np.zeros((net_dims[l], 1))\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tests\n",
    "params = initialize_params_he([5,4,3])\n",
    "params[\"W\"+str(2)].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glorot Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params_glorot(net_dims):   \n",
    "    np.random.seed(3)\n",
    "    params = {}\n",
    "    L = len(net_dims)            # number of layers in the network, including input\n",
    "    # Initialize params for each layer of the network, excluding the input layer\n",
    "    for l in range(1, L):\n",
    "        params['W' + str(l)] = np.random.randn(net_dims[l], net_dims[l-1]) * np.sqrt(2 / (net_dims[l-1] + net_dims[l]))\n",
    "        params['b' + str(l)] = np.zeros((net_dims[l], 1))\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 0.84316755,  0.20577272,  0.04548934, -0.87845888, -0.13076205],\n",
      "       [-0.16723499, -0.03900471, -0.29557095, -0.02065608, -0.22496274],\n",
      "       [-0.61936178,  0.41701499,  0.41545731,  0.80590047,  0.02358609],\n",
      "       [-0.19076676, -0.25708514, -0.7290164 ,  0.46309245, -0.51904826]]), 'b1': array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]]), 'W2': array([[-0.63343401, -0.1099245 ,  0.79437971,  0.12653017],\n",
      "       [-0.54723618, -0.3811109 ,  0.33420749, -0.085798  ],\n",
      "       [-0.41096032, -0.12295659,  0.39824933,  1.05627564]]), 'b2': array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]])}\n"
     ]
    }
   ],
   "source": [
    "#Tests\n",
    "params = initialize_params_glorot([5,4,3])\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeCun Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params_lecun(net_dims):\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    params = {}\n",
    "    L = len(net_dims)            # number of layers in the network, including input\n",
    "    # Initialize params for each layer of the network, excluding the input layer\n",
    "    for l in range(1, L):\n",
    "        params['W' + str(l)] = np.random.randn(net_dims[l], net_dims[l-1]) * np.sqrt(1 / (net_dims[l-1] + net_dims[l]))\n",
    "        params['b' + str(l)] = np.zeros((net_dims[l], 1))\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Combination Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_combination(A_prev, W, b):\n",
    "    Z = W.dot(A_prev) + b\n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    assert(A.shape == Z.shape)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, params, layer_activation=\"relu\", output_activation=\"sigmoid\", keep_prob=1):\n",
    "    \n",
    "    # list of cached values for backprop\n",
    "    cache_list = []\n",
    "    A = X\n",
    "    L = len(params) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Set up the activation functions\n",
    "    if(layer_activation==\"relu\"):\n",
    "        activation = relu\n",
    "    elif(layer_activation==\"sigmoid\"):\n",
    "        activation = sigmoid\n",
    "    else:\n",
    "        sys.exit(\"Sorry, but this activation function has not been implemented yet. Try another!\")\n",
    "        \n",
    "    if(output_activation==\"sigmoid\"):\n",
    "        final_activation = sigmoid\n",
    "    elif(output_activation==\"relu\"):\n",
    "        final_activation = relu\n",
    "    else:\n",
    "        sys.exit(\"Sorry, but this activation function has not been implemented yet. Try another!\")\n",
    "\n",
    "    \n",
    "    #Put Input Layer Forward Prop HERE, MODIFY for-loop to exclude it\n",
    "    \n",
    "    # Compute forward propagation for each layer except output layer\n",
    "    for l in range(1, L):\n",
    "        # grab the weight matrix and bias vector of the layer\n",
    "        W = params[\"W\" + str(l)]\n",
    "        b = params[\"b\" + str(l)]\n",
    "        A_prev = A \n",
    "        # compute linear combination \n",
    "        Z = linear_combination(A_prev, W, b)\n",
    "        # cache values for later use in backprop\n",
    "        cache_list.append((A_prev, W, b, Z))\n",
    "        # compute non-linear activation\n",
    "        A = activation(Z)\n",
    "\n",
    "    \n",
    "    # Output Layer Forward Propagation\n",
    "    W = params[\"W\" + str(L)]\n",
    "    b = params[\"b\" + str(L)]\n",
    "    A_prev = A\n",
    "    Z = linear_combination(A_prev, W, b)\n",
    "    AL = final_activation(Z)\n",
    "    cache_list.append((A_prev, W, b, Z))\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, cache_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(AL, Y):\n",
    "    # number of examples\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss\n",
    "    loss = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    loss = np.squeeze(loss)      # To make sure your cost's shape is what we expect\n",
    "    assert(loss.shape == ())\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Derivative Function (ADD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Derivative Functions (CHANGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_gradient(Z):\n",
    "    A = sigmoid(Z)\n",
    "    dA_dZ = A * (1 - A)\n",
    "    assert(dA_dZ.shape == Z.shape)\n",
    "    return dA_dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_gradient(Z):\n",
    "    dA_dZ = np.copy(Z)\n",
    "    dA_dZ[dA_dZ>0] = 1\n",
    "    dA_dZ[dA_dZ<=0] = 0\n",
    "    assert(dA_dZ.shape == Z.shape)\n",
    "    return dA_dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Combination Derivative Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_linear_combination(dZ, A_prev, W, b):\n",
    "\n",
    "    # number of training examples\n",
    "    m = A_prev.shape[1]\n",
    "    # compute gradient of loss wrt to the weight matrix of the network's given layer l: dL/dWl\n",
    "    dW = 1./m * np.matmul(dZ, A_prev.T)\n",
    "    # compute gradient of loss wrt to the bias vector of the network's given layer l: dL/dbl\n",
    "    db = 1./m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    # compute gradient of loss wrt to the activations of the previous layer l-1: dL/dAl-1\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(AL, Y, cache_list, layer_activation=\"relu\", output_activation=\"sigmoid\"):\n",
    "\n",
    "    # initialize dict for gradients\n",
    "    grads = {}\n",
    "    L = len(cache_list) # number of layers\n",
    "    m = AL.shape[1] # number of training examples\n",
    "    Y = Y.reshape(AL.shape) # Y is the same shape as AL\n",
    "    \n",
    "    # Set up the activation function gradients\n",
    "    if(layer_activation==\"relu\"):\n",
    "        activation_gradient = relu_gradient\n",
    "    elif(layer_activation==\"sigmoid\"):\n",
    "        activation_gradient = sigmoid_gradient\n",
    "    else:\n",
    "        sys.exit(\"Sorry, but this activation function gradient has not been implemented yet. Try another!\")\n",
    "        \n",
    "    if(output_activation==\"sigmoid\"):\n",
    "        final_activation_gradient = sigmoid_gradient\n",
    "    elif(output_activation==\"relu\"):\n",
    "        final_activation_gradient = relu_gradient\n",
    "    else:\n",
    "        sys.exit(\"Sorry, but this activation function gradient has not been implemented yet. Try another!\")\n",
    "    \n",
    "    # compute gradient of error wrt to activations of the output layer: dE/dA_[L]\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    grads[\"dA\" + str(L)] = dAL\n",
    "    \n",
    "# Backpropagation for Output Layer\n",
    "    # retrieve cached values for the last hidden layer\n",
    "    A_prev, W, b, Z = cache_list[L-1]\n",
    "    \n",
    "    # compute gradient of error wrt to Z_[L]\n",
    "    dZL = dAL * final_activation_gradient(Z)\n",
    "    \n",
    "    assert(dZL.shape == Z.shape)\n",
    "    \n",
    "    # compute gradient of error wrt to A_[L-1], W_[L], and b_[L]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = backward_linear_combination(dZL, A_prev, W, b)\n",
    "\n",
    "# Backpropagation for hidden layers    \n",
    "\n",
    "    # compute grads for layers L-1 to 1\n",
    "    for l in reversed(range(L-1)):\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        # retrieve cached values from previous layer\n",
    "        A_prev, W, b, Z = cache_list[l]\n",
    "        # compute gradient of loss wrt linear combination of layer l: dL/dZl\n",
    "        dZl = grads[\"dA\" + str(l+1)] * activation_gradient(Z)\n",
    "        # compute gradient of loss wrt to activations of the previous layer l-1, weights and biases of current layer\n",
    "        dA_prev_temp, dW_temp, db_temp = backward_linear_combination(dZl, A_prev, W, b)\n",
    "        # put the gradient values in our dict\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning: Updating Parameters through Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_params_sgd(params, grads, learning_rate, optimizer=\"SGD\", t=0, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    L = len(params) // 2 # number of layers\n",
    "    \n",
    "    for l in range(L):\n",
    "        params[\"W\" + str(l+1)] = params[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        params[\"b\" + str(l+1)] = params[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    return params            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_adam(parameters):\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize v, s.\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros_like(params[\"W\" + str(l+1)])\n",
    "        v[\"db\" + str(l+1)] = np.zeros_like(params[\"b\" + str(l+1)])\n",
    "            \n",
    "        s[\"dW\" + str(l+1)] = np.zeros_like(params[\"W\" + str(l+1)])\n",
    "        s[\"db\" + str(l+1)] = np.zeros_like(params[\"b\" + str(l+1)])\n",
    "        \n",
    "    return v, s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_params_adam(parameters, grads, v, s, t, learning_rate=0.01,\n",
    "                                beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    v_corrected = {}                         # Initializing first moment estimate\n",
    "    s_corrected = {}                         # Initializing second moment estimate\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "        \n",
    "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"dW\" + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "\n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1 - (beta1**t))\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1 - (beta1**t))\n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "    \n",
    "        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * np.square(grads[\"dW\" + str(l+1)])\n",
    "        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * np.square(grads[\"db\" + str(l+1)])\n",
    "        \n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        \n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1 - (beta2**t))\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1 - (beta2**t))\n",
    "        \n",
    "\n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "    \n",
    "        params[\"W\" + str(l+1)] = params[\"W\" + str(l+1)] - (learning_rate * v_corrected[\"dW\" + str(l+1)]/(np.sqrt(s_corrected[\"dW\" + str(l+1)]) + epsilon))\n",
    "        params[\"b\" + str(l+1)] = params[\"b\" + str(l+1)] - (learning_rate * v_corrected[\"db\" + str(l+1)]/(np.sqrt(s_corrected[\"db\" + str(l+1)]) + epsilon))\n",
    "        \n",
    "\n",
    "    return params, v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_mini_batches(X, Y, batch_size=64, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1]    # number of training examples\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y)\n",
    "    num_complete_minibatches = math.floor(m/batch_size) # number of complete mini batches \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        \n",
    "        mini_batch_X = shuffled_X[:,k * batch_size:(k + 1) * batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:,k * batch_size:(k + 1) * batch_size]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # If (last mini-batch < mini_batch_size)\n",
    "    if m % batch_size != 0:\n",
    "        \n",
    "        mini_batch_X = shuffled_X[:,num_complete_minibatches * batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:,num_complete_minibatches * batch_size:]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Our Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(X, Y, net_dims, initializer=\"random\", optimizer=\"sgd\", learning_rate=0.001, batch_size=64, num_epochs=1000, print_loss=False):\n",
    "    L = len(net_dims)             # number of layers in the neural networks, including input layer\n",
    "    losses = []                       # to keep track of the loss\n",
    "    cnter = 0                        # initializing the counter required for Adam update\n",
    "    seed = 5                         # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
    "    \n",
    "    # Initialize parameters\n",
    "    if(initializer==\"random\"):\n",
    "        params = initialize_params_random(net_dims)\n",
    "    elif(initializer==\"he\"):\n",
    "        params = initialize_params_he(net_dims)\n",
    "    elif(initializer==\"glorot\"):\n",
    "        params = initialize_params_glorot(net_dims)\n",
    "    elif(initializer==\"lecun\"):\n",
    "        params = initialize_params_lecun(net_dims)\n",
    "    else:\n",
    "        sys.exit(\"Sorry, this initialization method has not been implemented yet. Try another!\")\n",
    "        \n",
    "    # Initialize optimizer\n",
    "    if(optimizer==\"sgd\"):\n",
    "        update_params = update_params_sgd\n",
    "    elif(optimizer==\"adam\"):\n",
    "        update_params = update_params_adam\n",
    "        v, s = initialize_adam(params)\n",
    "    else:\n",
    "        sys.exit(\"Sorry, this optimizer has not been implemented yet. Try another!\")\n",
    "    \n",
    "    # Training loop\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # Increment seed to generate unique set of random batches for each epoch\n",
    "        seed = seed + 1\n",
    "        minibatches = generate_mini_batches(X, Y, batch_size, seed)\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            \n",
    "            minibatch_X.shape\n",
    "\n",
    "            # Forward propagation\n",
    "            AL, caches = forward_propagation(minibatch_X, params)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = compute_loss(AL, minibatch_Y)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = backward_propagation(AL, minibatch_Y, caches)\n",
    "\n",
    "            # Update parameters\n",
    "            if(optimizer==\"sgd\"):\n",
    "                params = update_params(params, grads, learning_rate)\n",
    "            elif(optimizer==\"adam\"):\n",
    "                cnter += 1\n",
    "                params = update_params(params, grads, v, s, learning_rate, beta1, beta2, epsilon, t=cnter)\n",
    "            else:\n",
    "                sys.exit(\"Sorry, this optimizer has not been implemented yet. Try another!\")\n",
    "            \n",
    "        # Print the cost every 1000 epoch\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print(\"Loss after epoch %i: %f\" % (i, loss))\n",
    "        if print_loss and i % 100 == 0:\n",
    "            losses.append(loss)\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(losses))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction for Binary Classification Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_predict(params, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    AL, cache = forward_propagation(X, params)\n",
    "    Yhat = (AL > 0.5)\n",
    "    predictions = Yhat\n",
    "    \n",
    "    return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
